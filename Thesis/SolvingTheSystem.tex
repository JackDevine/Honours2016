\epigraph{He who refuses to do arithmetic is doomed to talk nonsense.}{\textbf{John McCarthy \\ -- Progress and it's sustainability (1995)}}
%\setlength\epigraphwidth{.6\textwidth}
%\epigraph{People either write code that works, or they don't.}{\textbf{Colin Fox -- 2016}}
Now that we have determined that our system is physical and we have made insights into the qualitative nature of our system, we must build techniques for solving the system so that we can gain quantitative insights. We will begin in ~\autoref{SteadyState} by using analytical techniques to find stationary solutions to our equations of motion.
\section{Steady state solution} \label{SteadyState}
In order to see how the equations of motion behave with time, we have to resort to numerical methods (see section \ref{numerics}). However we note that for a given potential there will be a stationary solution that we will refer to as the ``steady state'', given periodic boundary conditions, we will derive an analytical form for this steady state.

In the steady state, we have:

\begin{eqnarray}
\frac{\partial P(x, t)}{\partial t} &=&  0 \ = \ \frac{\partial J}{\partial x} \label{eqn:SmoluchowskiSteady} \\
\frac{\partial T(x, t)}{\partial t} &=& 0 \ = \ -\kappa q(x,t) + \frac{\partial}{\partial x} \left ( D \frac{\partial T(x, t)}{\partial x} \right ) \label{eqn:TemperatureSteady}
\end{eqnarray}
Suppose that we have the following boundary conditions:
\begin{align}
P(x = 0) & = P(x = L) \\
J(x = 0) & = J(x = L) \\
\left. \frac{\partial T}{\partial x} \right \rvert_{x = 0} & = 0 = \left. \frac{\partial T}{\partial x} \right \rvert_{x = L} \label{eqn:temperatureBoundary}
\end{align}
where $L$ is the length of the domain. Physically, these conditions say that the nett current flowing out of the boundaries is zero and that no heat escapes from the system, thus the energy of the sytem is conserved. Section 5.2 of \cite{Gardiner2009} gives the steady state current as:

\begin{equation}
J_s = \left [\frac{2 k_B T(L)}{\psi(L)} - \frac{2 k_B T(0)}{\psi(0)}  \right] P_s(0) \left [\int_0^L dx'/\psi(x') \right]^{-1}
\label{eqn:SteadyCurrent}
\end{equation}
with $\psi(x) \equiv \exp[-\int_0^x dx' \frac{\partial_x V(x')}{2 k_B T(x')}]$. Meanwhile, the probability density is:

\begin{equation}
P_s(x) = P_s(0) \left [\frac{\int_0^x \frac{dx'}{\psi(x')} \frac{T(L)}{\psi(L)} + \int_x^L \frac{dx'}{\psi(x')} \frac{T(0)}{\psi(0)} }{\frac{T(x)}{\psi(x)} \int_0^L \frac{dx'}{\psi(x')} } \right]
\label{eqn:SteadyDensity}
\end{equation}
In this case, $J_s$ is a constant and $P_s(0)$ is also a constant. Assuming that we know these constants it is now possible to find the steady state temperature. We have:

\begin{equation}
\frac{\partial T}{\partial t} = 0 = -\kappa J_s \partial_x V + D \frac{\partial^2 T}{\partial x^2} \label{eqn:steadyTemperatureODE}
\end{equation}
In one dimension, equation \ref{eqn:steadyTemperatureODE} can be written as an ordinary differential equation of the form
\begin{equation}
T''(x) = \frac{\kappa J_s}{D} V'(x)
\end{equation}
We can solve this equation by integrating both sides twice to give:

\begin{equation}
T(x) = \frac{\kappa J_s}{D} \int_0^x V(x') dx' + \xi x + d \label{eqn:steadyTemperature}
\end{equation}
for unkown constants $\xi$ and $d$. By applying the boundary condition \ref{eqn:temperatureBoundary}, we find that
\begin{align}
T'(0) & = 0 = \frac{\kappa J_s}{D} V(0) + \xi \\
T'(L) & = 0 = \frac{\kappa J_s}{D} V(L) + \xi
\end{align}
This implies that either or $V(0) = V(L)$ or $J_s$ is a function of $x$, so in the tilted case, we find that the steady state current is a function of $x$. In the case of a titled periodic potential it is not sensible to think of a steady state solution because there will always be a flow of the probability distribution. This will be associated with  a flow of energy through the boundaries meaning that energy is no longer conserved.

On the other hand, if we have $V(0) = V(L)$, then we can solve for $\xi$ to give:
\begin{equation}
\xi = - \frac{\kappa J_s}{D} V(0)
\end{equation}
Later, we will see that it is possible to have a steady state in higher dimensions where the flow of heat from the environment can dissipate the heat produced by the Brownian particle. By recalling that $E = \int_0^L P(x') V(x') dx' + c_p \int_0^L T(x') dx'$ we are able to find an expression for $d$. First, we will integrate the temperature from $0$ to $L$
\begin{equation}
\int_0^L T(x') dx' = \frac{\kappa J_s}{D} \int_0^L dx' \int_0^{x'} V(x'') dx'' + \xi \frac{L^2}{2} + L d
\end{equation}
Therefore,
\begin{equation}
	d = \frac{1}{L} \left(c_p E - c_p \int_0^L P(x') V(x') dx' - \frac{\kappa J_s}{D} \int_0^L dx' \int_0^{x'} V(x'') d x'' + \xi \frac{L^2}{2} \right)
\end{equation}

It would seem that one should be able to calculate the steady state current and density directly from the equations shown above. However, we notice that the constants $J_s$ and $P_s(0)$ have to satisfy equations (\ref{eqn:SteadyCurrent}), (\ref{eqn:SteadyDensity}) and (\ref{eqn:steadyTemperature}) while also satisfying the normalization condition $\int_0^L P(x) dx = 1$. To do this we define an objective function given by

\begin{equation}
obj(J_s, P_s(0)) = \left (J_s - \left [\frac{2 k_B T(L)}{\psi(L)} - \frac{2 k_B T(0)}{\psi(0)}  \right] P_s(0) \left [\int_0^L dx'/\psi(x') \right]^{-1} \right)^2  \label{eqn:Objective}
\end{equation}

And we minimize this objective function with respect to $J_s$ and $P_s(0)$ under the constraint $\int_0^L P(x) dx = 1$. Another way to do this is to guess a steady state density and temperature and use finite differencing to simulate forward in time until the transients die out. As well as this, one can write down the system as an ordinary differential equation in one dimension, this coupled set of ODEs can be solved numerically using the Runge Kutte technique. An example plot of the steady state is shown in Figure \ref{fig:steadyODE}, here we see that in the steady state the temperature has become a straight line.
\begin{figure}
	\begin{subfigure}{0.75\textwidth}
		\includegraphics[width=\columnwidth]{steadyODE}
	\end{subfigure}
	\begin{subfigure}{0.23\textwidth}
		\includegraphics[width=\columnwidth]{landauersBlowtorchLegend}
	\end{subfigure}
	\caption{\textbf{A solution of the system in the steady state} \label{fig:steadyODE}}
\end{figure}
{\color{red} do you think that we should show some plots of the steady state? I have a few that I calculated by solving the ODE so it might be worthwhile includign them} 

%----------------------------------------------------------------------------------------
%	NUMERICS
%----------------------------------------------------------------------------------------
\section{Finite differences}  \label{numerics}

{\color{red} do you think that I should include some figures showing examples of the finite differences so that the reader is introduced to the idea of the dynamics before the exploration section?}
Our one dimensional equations can be solved on a discrete grid by using the finite differences method, the main idea behind this strategy is to approximate derivatives with equations of the form:

\begin{equation}
\frac{d f}{d x} \approx \frac{f(x - h) - f(x + h)}{2h}
\end{equation}
for some small $h$, likewise the second derivative of a function is approximated with
\begin{equation}
\frac{d^2 f}{d x^2} \approx \frac{f(x - h) - 2f(x) + f(x + h)}{h^2}
\end{equation}
In our simulations, we will use the Crank Nicolson scheme \cite{Crank1996,Press2007} to solve the equations. From now on, we will use the notation that $F(j \Delta x, n \Delta t) = F_j^n$, the key equation for the Crank Nicolson scheme is:
\begin{equation}
\frac{P_j^{n+1} - P_j^n}{\Delta t} = \frac{1}{2}(F_j^{n+1} + F_j^n)
\end{equation}
where $F$ represents the right hand side of the equation that we are doing finite differences on. This is an implicit set of equations that we will need to solve, in particular one may notice that $F$ needs to be estimated at the future time in order for the equation to make sense. We will linearize the problem by assuming that the quantities of interest do not change by much in one time step.

By applying finite differences to the dimensionless Smoluchowski equation (eq \ref{eqn:dimensionlessSmoluchowski}), we find that:
\begin{multline}
F_j^{i} = \frac{P^i_{j+1} \partial V^i_{j+1} - P^i_{j-1} \partial V^i_{j-1}}{2 \Delta x} + \frac{T^i_{j+1} - T^i_{j-1}}{2 \Delta x} \frac{P^i_{j+1} - P^i_{j-1}}{2 \Delta x} \\
 + T^i_j \frac{}{} \frac{P^i_{j+1}- 2P^i_j + P^i_{j-1}}{\Delta x^2} 
\end{multline}
where we omitted the hats for notational convenience.
We make the following definitions:
\begin{align*}
a_j^{n+1} &= \frac{-2 T^{n+1}_j}{\Delta x^2} \\
b_j^{n+1} &=  \frac{\partial_x V^{n+1}_{j+1}}{2\Delta x} + \frac{T^{n+1}_{j+1} - T^{n+1}_{j-1}}{4 \Delta x^2} \\
c_j^{n+1} &= -\frac{\partial_x V^{n+1}_{j-1}}{2\Delta x}  - \frac{T^{n+1}_{j+1} - T^{n+1}_{j-1}}{4 \Delta x^2} \\ 
a_j^{n} &= \frac{-2 T^{n}_j}{\Delta x^2} \\
b_j^{n} &=  \frac{\partial_x V^{n}_{j+1}}{2\Delta x} + \frac{T^{n}_{j+1} - T^{n}_{j-1}}{4 \Delta x^2} \\
c_j^{n} &= -\frac{\partial_x V^{n}_{j-1}}{2\Delta x}  - \frac{T^{n}_{j+1} - T^{n}_{j-1}}{4 \Delta x^2} \numberthis
\end{align*}
With these definitions, the Crank Nicolson scheme can be written down as follows:
\begin{multline}
 -\frac{\Delta t}{2}a_j^{n+1}P_{j-1}^{n+1} + \left (1 - \frac{\Delta t}{2}b_j^{n+1} \right) P_j^{n+1} - \frac{\Delta t}{2} c_j^{n+1} P_{j+1}^{n+1} \\
 = a_j^n P_{j-1}^{n}
+ \left (1 + \frac{\Delta t}{2}b_j^n \right) P_j^{n}  + \frac{\Delta t}{2} c_j^n P_{j+1}^{n}
\end{multline}
This equation can be written in matrix form by defining the following matrices:
\begin{equation}
A =
\begin{bmatrix}
	a_0^{n+1} & b_1^{n+1} & 0                & 0                & 0        & \dots                                 & 0        \\
	c_0^{n+1} & a_1^{n+1} & b_2^{n+1} & 0                & 0        & \dots                                 & 0        \\
	0                & c_1^{n+1} & a_2^{n+1} & b_3^{n+1}   & 0        & \dots                                 & 0        \\
			     &                   &                   &                  &           &                                         &           \\
	\vdots         & \vdots         & \ddots         & \ddots         & \ddots & \vdots                              & \vdots \\
			     &                   &                   &                  &           &                                         &           \\
			     &                   &                   &                  &           &                                         &           \\
	0                &                   & \dots           &                   &  c_{J-2}^{n+1} & a_{J-1}^{n+1}  & b_J^{n+1} \\
	0                &                   & \dots           &                   &                         &  c_{J-1}^{n+1} & a_J^{n+1}
\end{bmatrix}
,\quad P^{n+1} =
\begin{bmatrix}
P_0^{n+1}       \\
P_1^{n+1}	     \\
                        \\
                        \\
\vdots               \\
                        \\
                        \\
P_{J-1}^{n+1} \\
P_J^{n+1}
\end{bmatrix}
\label{eqn:matrix}
\end{equation}

\begin{equation}
B =
\begin{bmatrix}
	a_0^{n} & b_1^{n}     & 0                 & 0          & 0                    & \dots            & 0        \\
	c_0^{n} & a_1^{n}     & b_2^{n}      & 0          & 0                    & \dots            & 0        \\
	0                & c_1^{n} & a_2^{n}      & b_3{n} & 0                    & \dots            & 0        \\
			     &               &                   &             &                      &                     &           \\
	\vdots         & \vdots     & \ddots         & \ddots   & \ddots            & \vdots           & \vdots \\
			     &               &                   &             &                      &                     &           \\
			     &               &                   &             &                      &                     &           \\
	0                &               & \dots           &             &  c_{J-2}^{n} & a_{J-1}^{n}  & b_J^{n} \\
	0                &               & \dots           &             &                     &  c_{J-1}^{n} & a_J^{n}
\end{bmatrix}
,\quad P^{n} =
\begin{bmatrix}
P_0^{n}       \\
P_1^n          \\
                    \\
                    \\
\vdots           \\
                    \\
                    \\
P_{J-1}^{n} \\
P_J^{n}
\end{bmatrix}
\end{equation}
With these matrices, the equation now becomes,
\begin{equation}
\left (\mathbb{1} - \frac{\Delta t}{2}A \right) \cdot P^{n+1} = \left (\mathbb{1} + \frac{\Delta t}{2}B \right) \cdot P^n
\end{equation}
we interpret this equation as saying that half of a backwards Euler step acting on $P^{n+1}$ is equal to half of a forward Euler step acting on $P^n$. We write the equation to step $P$ forward one time step as
\begin{equation}
P^{n+1} = \frac{\mathbb{1} + \frac{\Delta t}{2}B}{\mathbb{1} - \frac{\Delta t}{2}A} P^n
\end{equation}
Each time that we step forward using this equation we will be out by a factor, this means that at each step we will need to renormalize using the equation $\int P(x) dx = 1$. If we do not apply this normalization, then the norm of our vector will change dramatically during a simulation. In order to estimate the integrals involved, we used the Simpson's rule of integration.

Likewise, we can apply the Crank Nicolson scheme to the heat equation, (eq \ref{eqn:dimensionlessHeat}), by looking at the right hand side of this equation, we find that

\begin{equation}
F_j^i = -\alpha \left (P_j^i (\partial_x V_j^i)^2 + T_j^i \frac{P_{j+1}^i - P_{j-1}^i }{2 \Delta x}\partial_x V_j^i  \right) + \beta \frac{T_{j+1}^i - 2T_j^i + T_{j-1}^i}{\Delta x^2}
\end{equation}

Just like the discretized Smoluchowski equation, these equations can be written in matrix form. The temperature is normalized by assuming that the energy remains fixed, this will be true as long as no heat or current flows through the boundaries, i.e. $J(x=a) = 0 = J(x = b)$ and $\frac{\partial T}{\partial x} \rvert_a = 0 = \frac{\partial T}{\partial x} \rvert_b$. In this case, the energy is constant and is given by $E = \int P(x) V(x) dx + c_p \int T(x) dx$, so each time that we step the temperature forward, we have to calculate the potential and thermal energy and then scale the temperature so that the total energy remains fixed.

Fortunately the matrices that we are dealing with are very sparse, so the program used to solve these equations can save on memory by calling sparse matrix libraries.

\subsection{Boundary conditions}
So far we have discussed how to solve the equations inside the domain of interest $\Omega$, however our equations of motion will require that we prescribe boundary conditions that the system must satisfy. The boundary conditions that have been implemented for this project are as follows:

\begin{itemize}
\item{Dirichlet: The value of the solution is specified at the boundaries}
\item{Neumann: The value of the first derivative of the solution is specified at the boundaries}
\item{Periodic: The value at the boundaries is not specified, but the left and right boundaries must take on the same value}
\end{itemize}
Each type of boundary conditions comes along with its own physical interpretation, first we will deal with the boundary conditions imposed on the probability distribution.

If the potential goes to infinity at the boundaries, then the probability distribution must vanish at the boundaries before applying boundary conditions at all, therefore no boundary conditions need to be applied. There are some potentials that are periodic where the value of the potential is the same finite number at both sides, in this case it can be interesting to apply periodic boundary conditions to the probability distribution. Physically, this corresponds to a system where a particle passing through the left side will appear at the right side and vice-versa. One can picture this by imagining a particle constrained to a circular domain.

As for the temperature, both Neumann and Dirichlet boundary conditions are realized. In the case of Neumann boundary conditions, the derivative was set to zero at both boundaries. This is a very interesting case to imagine, because the derivative of the temperature physically represents the flow of heat in our system. Therefore the physical interpretation of a vanishing derivative at the boundary is that no heat is flowing through the boundaries, or in other words the system is enclosed in a perfectly insulating box. This system conserves energy, which is convenient when one goes to renormalize the calculated temperature by using the energy of the system.

The second type of boundary condition that can be imposed on the temperature is the Dirichlet type where the value of the temperature is specified at both boundaries. The physical interpretation of this requirement is that the domain is embedded in a much larger system that is at a fixed temperature. The tacit assumption made is that no matter what the system does, it is not able to affect the temperature of the environment. One must take care when using the energy to renormalize when considering Dirichlet boundary conditions, this is because the energy in the system is no longer conserved. To account for this, we calculate the heat flowing through the boundaries at each step and subtract this from the energy of the system before we renormalized the system.

Implementing these boundary conditions requires that we change our matrices $A$ and $B$, specifically we will need to modify the top and bottom rows. The reason that we can do this is because a matrix represents a system of equations, the first and last rows of the matrices effect the first and last components of the vectors that we are doing finite differences on. For each boundary condition, we need to make sure that the equations represented by our first and last rows reflect our desired boundary conditions.

\subsubsection{Dirichlet boundary conditions}
Imagine that we have a $n \times n$ vector $u$ that we want to step forward with finite differences while keeping the first and last components of $u$ fixed. The equation for stepping $u$ forward is given by:

\begin{equation}
u^{n+1} = \frac{\mathbb{1} + \frac{\Delta t}{2} B}{\mathbb{1} - \frac{\Delta t}{2} A} u^n
\end{equation}

This equation will certainly affect the first and last components of $u$, in order to avoid this, we need to make sure that the first and last rows of $(\mathbb{1} + \frac{\Delta t}{2} B)$ and $(\mathbb{1} - \frac{\Delta t}{2} A)$ are the same as the first and last rows of the identity matrix.

\subsubsection{Neumann boundary conditions}
The derivative at the boundaries can be made to be zero at the boundaries by making sure that the first and last rows of our matrices represent the equations $u_J - u_{J-1} = 0$ and $u_0 - u_1 = 0$ respectively. This can be done by replacing the first row with $\{ 1, \, -1, \, 0, \, \dots \, , \, 0 \}$ and the last row with $\{0, \, 0, \, \dots, \, 0, \, -1, \, 1 \}$.

\subsubsection{Periodic boundary conditions}
Periodic boundary conditions state that $u_0 = u_J$, if this is true at step $i$, then we can make sure that it is true at step $i+1$ by modifying our matrix appropriately. To be more precise if our finite differences matrix takes the form of equation \ref{eqn:matrix}, then in order to have periodic boundary conditions we would have to modify it to read:

\begin{equation}
A =
\begin{bmatrix}
	a_0^{n+1} & b_1^{n+1} & 0                & 0                & 0        & \dots                                 & c_0^{n+1}        \\
	c_0^{n+1} & a_1^{n+1} & b_2^{n+1} & 0                & 0        & \dots                                 & 0        \\
	0                & c_1^{n+1} & a_2^{n+1} & b_3^{n+1}   & 0        & \dots                                 & 0        \\
			     &                   &                   &                  &           &                                         &           \\
	\vdots         & \vdots         & \ddots         & \ddots         & \ddots & \vdots                              & \vdots \\
			     &                   &                   &                  &           &                                         &           \\
			     &                   &                   &                  &           &                                         &           \\
	0                &                   & \dots           &                   &  c_{J-2}^{n+1} & a_{J-1}^{n+1}  & b_J^{n+1} \\
	b_J^{n+1}                &                   & \dots           &                   &                         &  c_{J-1}^{n+1} & a_J^{n+1}
\end{bmatrix}
\end{equation}
This is equivalent to stretching out the matrix in a periodic fashion, we note that the matrix for periodic boundary conditions cannot be written in tridiagonal form. When the matrices involved are tridiagonal, we can inform the compiler of our program so that it can used highly optimized routines for solving the equations. However, in the case of periodic boundary conditions, we simply inform the compiler that the matrices are sparse, this means that we do not get the same speed for periodic boundary conditions as we do for other boundary conditions. 

All of these boundary conditions were implemented in Julia \cite{Bezanson2014}, the user can decide which boundary condition that they want through a keyword argument and then the program will implement finite differences for that boundary type.

\section{Testing the numerics}

The idea behind finite differences is that as the discretization size goes to zero, the numerical approximation should converge on the correct analytical solution. Here we will compare our numerics with some known analytical results as well as performing convergence tests.
\subsection{A comparison with analytical results}

The Smoluchowski equation has a steady state probability density that takes on the form
\begin{equation}
P_{ss}(x) = N \exp{\left(-\int_a^x \frac{V'(x')}{T(x')} dx' \right)} \label{eqn:analSteadyState}
\end{equation}

Figure \ref{fig:smoluchowskiCompare} shows the a simulation where we began the system in the analytically calculated steady state, we then used finite differences to step forward 50,000 steps with $\Delta t = 10^{-4}$. After this simulation, we only found a minimal divergence from the steady state.
\begin{figure}
	\begin{subfigure}{0.49\textwidth}
	\includegraphics[width=\columnwidth]{smoluchowskiAnalytic}
	\end{subfigure}
	\begin{subfigure}{0.49\textwidth}
	\includegraphics[width=\columnwidth]{smoluchowskiAnalyticNormDiff}
	\end{subfigure}
	\caption{\textbf{Finite differences in the steady state.} The steady state for the system was calculated using equation ~\ref{eqn:analSteadyState}, we start the system off in this state and then simulate the system forward 50,000 steps with $\Delta t = 10^{-4}$ and $\Delta x = 0.02$. (a) shows the analytical steady state with the state after the simulation, (b) shows the absolute difference between these two vectors. Even after 50,000 steps the system has not deviated from the analytical steady state significantly. \label{fig:smoluchowskiCompare}}
\end{figure}

The heat equation can be solved using a Fourier series technique, in the case where there are no sources of heat, we have
\begin{equation}
\frac{\partial T}{\partial t} = \beta \frac{\partial^2 T}{\partial x^2}
\end{equation}
the initial condition for the temperature will be denoted by,
\begin{equation}
T(x, 0) = f(x)
\end{equation}

Lets say that the boundaries are at $x = \pm \infty$ and that the derivative of the temperature is zeros at the boudaries. The solution to the heat equation is given by:

\begin{equation}
T(x, t) = \sum_{n=1}^\infty D_n \sin \left(\frac{n \pi x}{L} \right) \exp\left(\frac{-n^2 \pi^2 \beta t}{L^2}\right)  \label{eqn:analTemperature}
\end{equation}
where
\begin{equation}
D_n = \frac{2}{L} \int_0^L f(x) \sin \left(\frac{n \pi x}{L} \right) dx
\end{equation}
Say that we begin with a Gaussian function for the temperature, so that:
\begin{equation}
T(x, \, 0) = f(x) =  \exp{\left (-\frac{x^2}{4 \beta} \right)}
\end{equation}
then as time goes on, the solution will be given by:
\begin{equation}
T(x, \, t) = \frac{1}{\sqrt{t + 1}} \exp{ \left(-\frac{x^2}{4 \beta (t + 1)} \right)}
\end{equation}
We can compare these analytical results to the numerical ones obtained through finite differences, this is done in Figures ~\ref{fig:smoluchowskiCompare} and \ref{fig:temperatureCompare}. These figures show that even after many steps, the calculated solution has not deviated from the analytical one.

\begin{figure}
	\begin{subfigure}{0.49\textwidth}
	\includegraphics[width=\columnwidth]{temperatureAnalytic}
	\end{subfigure}
	\begin{subfigure}{0.49\textwidth}
	\includegraphics[width=\columnwidth]{temperatureAbsDiff}
	\end{subfigure}
	\caption{\textbf{Finite differences on the heat equation.} We simulate a situation where there are no sources for the heat equation and use equation \ref{eqn:analTemperature} to obtain the analytical result we then compare this to the result of finite differences with $\Delta t = 10^{-2}$ and $\Delta x = 0.02$. \label{fig:temperatureCompare}}
\end{figure}

\subsection{Convergence tests}
If the finite differences methods that we have implemented are correct, then as the discretization size goes to zero, the numerical approximation will converge on the correct solution to the underlying equation being approximated. In the previous section we showed that finite differences approximated the solution very closely in some instances where the analytical result could be obtained. In general, we will not have an analytical solution to compare to but we would still like to be able to quantify the performance of our techniques.

Convergence tests involve decreasing the discretization size and checking whether the numerical solutions converge at all. In Figure \ref{fig:Convergence}, the Smoluchowski equation was simulated while keeping the temperature fixed, each time we halve $\Delta t$ and measure the normed difference between the new result and the previous one.

%Here we will decrease the step size and see whether or not the numerical scheme converges on a particular result

\begin{figure}
	\begin{subfigure}{0.49\textwidth}
		\includegraphics[width=\columnwidth]{probabilityConvergence}
	\end{subfigure}
	\begin{subfigure}{0.49\textwidth}
		\includegraphics[width=\columnwidth]{probabilityConvergenceRate}
	\end{subfigure}
\caption{\textbf{The convergence of the probability distribution.} As $\Delta t$ is decreased, the spatial discretization $\Delta x$ is kept constant at 0.006. (a) the Smoluchoski equation is simulated forward for 1.0 seconds each line shows the result with a different value of the time step $\Delta t$. (b) the normed difference between each of the successive vectors is calculated and the result is plotted on  a log scale, the slope of this graph is called the convergence rate.}
\label{fig:Convergence}
\end{figure}

Likewise, we can do convergence tests for the coupled system, for brevity we have only included the results for the evolution of the temperature. All of these plots show that the approximation converge exponentially to a particular solution, the convergence tests themselves do not prove that the numerical algorithms properly represent the equations ref{eqn:dimensionlessSmoluchowski} and \ref{eqn:dimensionlessHeat}. However, combined with our analytical tests, they give us confidence that the numerics do agree with the true solution.

\begin{figure}
	\begin{subfigure}{0.49\textwidth}
		\includegraphics[width=\columnwidth]{temperatureConvergence}
	\end{subfigure}
	\begin{subfigure}{0.49\textwidth}
		\includegraphics[width=\columnwidth]{temperatureNormDiff}
	\end{subfigure}
\caption{\textbf{The convergence of the temperature.} As $\Delta t$ is decreased, the spatial discretization $\Delta x$ is kept constant at 0.006. (a) the coupled equations are simulated forward for 1.0 seconds each line shows the temperature with a different value of the time step $\Delta t$. (b) the normed difference between each of the successive vectors is calculated and the result is plotted on  a log scale.}
\label{fig:temperatureConvergence}
\end{figure}

One can also perform convergence tests by increasing the number of points, from this one learns how sensitive the numerics are to the size of the spatial discretization $\Delta x$. This is shown in Figure \ref{fig:spaceConvergence}, here we see that the temperature converges much more rapidly than the probability distribution does, however as $\Delta x$ goes down, both converge exponentially.

\begin{figure}
	\center{
	\begin{subfigure}{0.5\textwidth}
		\includegraphics[width=\columnwidth]{spaceConvergenceInit}
		\caption{}
	\end{subfigure}
	}

	\begin{subfigure}{0.49\textwidth}
		\includegraphics[width=\columnwidth]{spaceConvergenceDensity}
		\caption{}
	\end{subfigure}
	\begin{subfigure}{0.49\textwidth}
		\includegraphics[width=\columnwidth]{spaceConvergenceTemperature}
		\caption{}
	\end{subfigure}

	\begin{subfigure}{0.49\textwidth}
		\includegraphics[width=\columnwidth]{densityConvergenceSpace}
		\caption{}
	\end{subfigure}
	\begin{subfigure}{0.49\textwidth}
		\includegraphics[width=\columnwidth]{temperatureConvergenceSpace}
		\caption{}
	\end{subfigure}
\caption{\textbf{Convergence tests for $\Delta x$.} $\Delta t$ is held fixed at $1 \times 10^{-5}$ and $\alpha = 1 \times 10^{-2} m^{-1}$ and $\beta = 0.1 m^2 kg^{-1}$. (a) The initial configuration of the system, both the potential and the initial temperature are chosen to be complicated functions with large derivatives so that we can see how sensitive the numerics are to gradients. (b, c) The system after 40,000 steps forward in time, here we have not used the filling plot style to show the probability distribution so that one can see the difference between the different lines. (d) For each vector that was found through finite differences, we calculate its difference from the previous one, the $x$ axis shows the length of each of the vectors. The $y$ axis shows the log of these differences (e) Convergence of the temperature with an increasing number of points. \label{fig:spaceConvergence}}
\end{figure}