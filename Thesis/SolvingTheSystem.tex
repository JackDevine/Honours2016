\epigraph{He who refuses to do arithmetic is doomed to talk nonsense.}{\textbf{John McCarthy \\ -- Progress and it's Sustainability (1995)}}
%\setlength\epigraphwidth{.6\textwidth}
%\epigraph{People either write code that works, or they don't.}{\textbf{Colin Fox -- 2016}}
Now that we have determined that our system is physical and we have made insights into the qualitative nature of our system, we must build techniques for solving the system so that we can gain quantitative insights. We will begin in ~\autoref{SteadyState} by using analytical techniques to find stationary solutions to our equations of motion.
\section{Steady state solution} \label{SteadyState}
In order to see how the equations of motion behave with time, we have to resort to numerical methods (see \autoref{numerics}). However, we note that for a given potential there will be a stationary solution that we will refer to as the ``steady state''. In the steady state we have
\begin{equation}
\frac{\partial P(x, \, t)}{\partial t} = 0 = \frac{\partial J(x, \, t)}{\partial t},
\end{equation}
this implies that the current in the steady state is constant, we will denote this with $J_s$. In the steady state, the temperature is not changing, so
\begin{equation}
\frac{\partial T(x, \, t)}{\partial t} = 0 = -\kappa J_s \partial_x V(x, \, t) + D \frac{\partial^2 T(x, \, t)}{\partial x^2}. \label{eqn:steadyTemp}
\end{equation}
In one dimension, equation \ref{eqn:steadyTemp} can be written as an ordinary differential equation of the form
\begin{equation}
T''(x) = \frac{\kappa J_s}{D} V'(x)
\end{equation}
We can solve this equation by integrating both sides twice to give
\begin{equation}
T(x) = \frac{\kappa J_s}{D} \int_0^x V(x') dx' + \xi x + d, \label{eqn:steadyTemperature}
\end{equation}
for unknown constants $\xi$ and $d$. If $J_s = 0$ then the steady state for the probability density is given by
\begin{equation}
P_{ss}(x) = N \exp{\left(-\int_a^x \frac{V'(x')}{k_B T(x')} dx' \right)}
\end{equation}
and
\begin{equation}
T(x) = \xi x + d.
\end{equation}
We can now apply boundary conditions to the temperature, these can be either Neumann
\begin{eqnarray}
\left.\frac{\partial T}{\partial x}\right\rvert_{\partial \Omega} = 0,
\end{eqnarray}
or Dirichlet,
\begin{equation}
T \rvert_{\partial \Omega} = T_0.
\end{equation}
In the case of Neumann boundary conditions, $\xi = 0$ since the slope is zero at the boundaries. For Dirichlet boundary conditions, $\xi = 0$ and $d = T_0$ both of these imply that the temperature is a constant which we write as $T$. In this case, equation \ref{eqn:steadyTemperature} becomes the Boltzmann distribution
\begin{equation}
P(x) = N \exp{\left (-\frac{V(x)}{k_B T} \right)}.
\end{equation}
An example of the Boltzmann distribution is shown in Figure \ref{fig:steadyODE}, in this figure we have a confining potential causing the probability density to go to zero for large $x$. It is interesting to note that in the steady state, the temperature gradients become flat. This is because after enough time the diffusion term in equation \ref{eqn:TemperatureEvolution} causes temperature gradients to flatten out.

In the case of a tilted periodic potential $V(x) = -f x + v_0(x)$ for some constant $f$ and a periodic function $v_0(x)$, for the tilted periodic potential the boundaries are at infinity and $V(x) \to -\infty$ as $x \to \infty$. In this case, equation \ref{eqn:steadyTemperature} becomes
\begin{equation}
T(x) = \frac{\kappa J_s}{D} \int_0^x -f x' dx' + \frac{\kappa J_s}{D} \int_0^x v_0(x') dx' + \xi x + d.
\end{equation}
The first term on the right hand side of this equation will grow like $x^2$. The reason that the temperature is growing is because as the Brownian particle moves down the potential, it loses potential energy. This potential energy is converted into thermal energy. Given that we cannot allow the temperature to diverge in this fashion, we conclude that the tilted periodic potential does not have a meaningful steady state.
%In the case of a confined potential, this steady state is given by the Boltzmann distribution
%\begin{equation}
%P(x) = N \exp(\frac{V(x)}{k_B T}).
%\end{equation}
%For a given potential and temperature, one can see that
%\begin{equation}
%P_{ss}(x) = N \exp{\left(-\int_a^x \frac{V'(x')}{T(x')} dx' \right)},
%\end{equation}
%will also cause the current in equation \ref{eqn:Smoluchowski} to become zero. In our case the steady state requires that the current is a constant which we will write as $J_s$ and that
%\begin{equation}
%\frac{\partial T(x, \, t)}{\partial t} = 0 = -\kappa J_s \partial_x V(x, \, t) + D \frac{\partial^2 T(x, \, t)}{\partial x^2}. \label{eqn:steadyTemp}
%\end{equation}
%In one dimension, equation \ref{eqn:steadyTemp} can be written as an ordinary differential equation of the form
%\begin{equation}
%T''(x) = \frac{\kappa J_s}{D} V'(x)
%\end{equation}
%We can solve this equation by integrating both sides twice to give
%\begin{equation}
%T(x) = \frac{\kappa J_s}{D} \int_0^x V(x') dx' + \xi x + d, \label{eqn:steadyTemperature}
%\end{equation}
%for unknown constants $\xi$ and $d$. If we apply periodic boundary conditions so that $T(x = 0) = T(x = L)$ for some $T_0$ then,
%\begin{eqnarray}
%T(x = 0) &=& 0 + 0 + d \\
%T(x = L) &=& \frac{-\kappa J_s}{D} \int_0^L V(x') dx' + \xi L + d,
%\end{eqnarray}
%this implies that $d = T_0$ and that
%\begin{equation}
%\xi = \frac{-\kappa J_s}{D L} \int_0^L V(x') dx'.
%\end{equation}
%Putting this all together we find that the temperature in the steady state is simply s straight line, this should be expected because straight lines are steady-state solutions to the heat equation without sources. Interestingly, we find that tilted potentials do not admit steady state solutions when the Smoluchowski equation is coupled to the heat equation. To see why not, consider a probability distribution that is flowing down a tilted potential, the energy of the system is given by equation \ref{eqn:energy}, this energy must be conserved. Normally, the steady-state in the tilted case is defined by the case where the current is constant. If this is the case, then as the probability distribution flows down the potential, the potential energy will decrease which must be compensated by an increase in thermal energy. This increase in thermal energy corresponds to a rise in the temperature, therefore $\frac{\partial T(x, \, t)}{\partial t} \ne 0$. 

%In the steady state, we have:
%
%\begin{eqnarray}
%\frac{\partial P(x, t)}{\partial t} &=&  0 \ = \ \frac{\partial J}{\partial x} \label{eqn:SmoluchowskiSteady} \\
%\frac{\partial T(x, t)}{\partial t} &=& 0 \ = \ -\kappa q(x,t) + \frac{\partial}{\partial x} \left ( D \frac{\partial T(x, t)}{\partial x} \right ) \label{eqn:TemperatureSteady}
%\end{eqnarray}
%Suppose that we have the following boundary conditions:
%\begin{align}
%P(x = 0) & = P(x = L) \\
%J(x = 0) & = J(x = L) \\
%\left. \frac{\partial T}{\partial x} \right \rvert_{x = 0} & = 0 = \left. \frac{\partial T}{\partial x} \right \rvert_{x = L} \label{eqn:temperatureBoundary}
%\end{align}
%where $L$ is the length of the domain. Physically, these conditions say that the net current flowing out of the boundaries is zero and that no heat escapes from the system, thus the energy of the sytem is conserved. Section 5.2 of \cite{Gardiner2009} gives the steady state current as:
%
%\begin{equation}
%J_s = \left [\frac{2 k_B T(L)}{\psi(L)} - \frac{2 k_B T(0)}{\psi(0)}  \right] P_s(0) \left [\int_0^L dx'/\psi(x') \right]^{-1}
%\label{eqn:SteadyCurrent}
%\end{equation}
%with $\psi(x) \equiv \exp[-\int_0^x dx' \frac{\partial_x V(x')}{2 k_B T(x')}]$. Meanwhile, the probability density is:
%
%\begin{equation}
%P_s(x) = P_s(0) \left [\frac{\int_0^x \frac{dx'}{\psi(x')} \frac{T(L)}{\psi(L)} + \int_x^L \frac{dx'}{\psi(x')} \frac{T(0)}{\psi(0)} }{\frac{T(x)}{\psi(x)} \int_0^L \frac{dx'}{\psi(x')} } \right]
%\label{eqn:SteadyDensity}
%\end{equation}
%In this case, $J_s$ is a constant and $P_s(0)$ is also a constant. Assuming that we know these constants it is now possible to find the steady state temperature. We have:
%
%\begin{equation}
%\frac{\partial T}{\partial t} = 0 = -\kappa J_s \partial_x V + D \frac{\partial^2 T}{\partial x^2} \label{eqn:steadyTemperatureODE}
%\end{equation}
%In one dimension, equation \ref{eqn:steadyTemperatureODE} can be written as an ordinary differential equation of the form
%\begin{equation}
%T''(x) = \frac{\kappa J_s}{D} V'(x)
%\end{equation}
%We can solve this equation by integrating both sides twice to give:
%
%\begin{equation}
%T(x) = \frac{\kappa J_s}{D} \int_0^x V(x') dx' + \xi x + d \label{eqn:steadyTemperature}
%\end{equation}
%for unknown constants $\xi$ and $d$. By applying the boundary condition \ref{eqn:temperatureBoundary}, we find that
%\begin{align}
%T'(0) & = 0 = \frac{\kappa J_s}{D} V(0) + \xi \\
%T'(L) & = 0 = \frac{\kappa J_s}{D} V(L) + \xi
%\end{align}
%This implies that either or $V(0) = V(L)$ or $J_s$ is a function of $x$, so in the tilted case, we find that the steady state current is a function of $x$. In the case of a tilted periodic potential it is not sensible to think of a steady state solution because there will always be a flow of the probability distribution. This will be associated with  a flow of energy through the boundaries meaning that energy is no longer conserved.
%
%On the other hand, if we have $V(0) = V(L)$, then we can solve for $\xi$ to give:
%\begin{equation}
%\xi = - \frac{\kappa J_s}{D} V(0)
%\end{equation}
%%Later, we will see that it is possible to have a steady state in higher dimensions where the flow of heat from the environment can dissipate the heat produced by the Brownian particle.
%By recalling that $E = \int_0^L P(x') V(x') dx' + c_p \int_0^L T(x') dx'$ we are able to find an expression for $d$. First, we will integrate the temperature from $0$ to $L$
%\begin{equation}
%\int_0^L T(x') dx' = \frac{\kappa J_s}{D} \int_0^L dx' \int_0^{x'} V(x'') dx'' + \xi \frac{L^2}{2} + L d
%\end{equation}
%Therefore,
%\begin{equation}
%	d = \frac{1}{L} \left(c_p E - c_p \int_0^L P(x') V(x') dx' - \frac{\kappa J_s}{D} \int_0^L dx' \int_0^{x'} V(x'') d x'' + \xi \frac{L^2}{2} \right)
%\end{equation}
%
%It would seem that one should be able to calculate the steady state current and density directly from the equations shown above. However, we notice that the constants $J_s$ and $P_s(0)$ have to satisfy equations (\ref{eqn:SteadyCurrent}), (\ref{eqn:SteadyDensity}) and (\ref{eqn:steadyTemperature}) while also satisfying the normalization condition $\int_0^L P(x) dx = 1$. To do this we define an objective function given by
%
%\begin{equation}
%\mathrm{obj}(J_s, P_s(0)) = \left (J_s - \left [\frac{2 k_B T(L)}{\psi(L)} - \frac{2 k_B T(0)}{\psi(0)}  \right] P_s(0) \left [\int_0^L dx'/\psi(x') \right]^{-1} \right)^2  \label{eqn:Objective}
%\end{equation}
%
%And we minimize this objective function with respect to $J_s$ and $P_s(0)$ under the constraint $\int_0^L P(x) dx = 1$. Another way to do this is to guess a steady state density and temperature and use finite differencing to simulate forward in time until the transients die out. As well as this, one can write down equations \ref{eqn:Smoluchowski} and \ref{eqn:TemperatureEvolution} as an ordinary differential equation in one dimension, this coupled set of ODEs can be solved numerically using the Runge-Kutta technique. An example plot of the steady state is shown in Figure \ref{fig:steadyODE}, here we see that in the steady state the temperature has become a straight line.
\begin{figure}
	\center
	\begin{subfigure}{0.75\textwidth}
		\includegraphics[width=\columnwidth]{steadyODE}
	\end{subfigure}
	\begin{subfigure}{0.23\textwidth}
		\includegraphics[width=\columnwidth]{landauersBlowtorchLegend}
	\end{subfigure}
	\caption{\textbf{A solution of the system in the steady state with a confining potential.} \label{fig:steadyODE}}
\end{figure}

%----------------------------------------------------------------------------------------
%	NUMERICS
%----------------------------------------------------------------------------------------
\section{Finite differences}  \label{numerics}
In order to explore the dynamics of equations \ref{eqn:Smoluchowski} and \ref{eqn:TemperatureEvolution} we will need to resort to numerical techniques. The one dimensional equations can be solved on a discrete grid by using the finite differences method. The main idea behind this strategy is to approximate derivatives with equations of the form

\begin{equation}
\frac{d f}{d x} \approx \frac{f(x - h) - f(x + h)}{2h},
\end{equation}
for some small $h$, likewise the second derivative of a function is approximated with
\begin{equation}
\frac{d^2 f}{d x^2} \approx \frac{f(x - h) - 2f(x) + f(x + h)}{h^2},
\end{equation}
In our simulations, we will use the Crank-Nicolson scheme \cite{Crank1996,Press2007} to solve the equations. From now on, we will use the notation that $F(j \Delta x, n \Delta t) = F_j^n$, the key equation for the Crank-Nicolson scheme is
\begin{equation}
\frac{P_j^{n+1} - P_j^n}{\Delta t} = \frac{1}{2}(F_j^{n+1} + F_j^n),  \label{eqn:crankNicolson}
\end{equation}
where $F$ represents the right-hand side of the equation that we are doing finite differences on. Equation \ref{eqn:crankNicolson} represents the average of taking one Euler step forwards and on Euler step backwards. This is an implicit set of equations that we will need to solve. In particular, one may notice that $F$ needs to be estimated at the future time in order for the equation to make sense. We will linearize the problem by assuming that the quantities of interest do not change by much in one time step.

By applying finite differences to the dimensionless Smoluchowski equation (equation \ref{eqn:dimensionlessSmoluchowski}), we find that:
\begin{multline}
F_j^{i} = \frac{P^i_{j+1} \partial V^i_{j+1} - P^i_{j-1} \partial V^i_{j-1}}{2 \Delta x} + \frac{T^i_{j+1} - T^i_{j-1}}{2 \Delta x} \frac{P^i_{j+1} - P^i_{j-1}}{2 \Delta x} \\
 + T^i_j \frac{}{} \frac{P^i_{j+1}- 2P^i_j + P^i_{j-1}}{\Delta x^2},
\end{multline}
where we omitted the hats for notational convenience.
We make the following definitions:
\begin{align*}
a_j^{n+1} &= \frac{-2 T^{n+1}_j}{\Delta x^2}, \\
b_j^{n+1} &=  \frac{\partial_x V^{n+1}_{j+1}}{2\Delta x} + \frac{T^{n+1}_{j+1} - T^{n+1}_{j-1}}{4 \Delta x^2}, \\
c_j^{n+1} &= -\frac{\partial_x V^{n+1}_{j-1}}{2\Delta x}  - \frac{T^{n+1}_{j+1} - T^{n+1}_{j-1}}{4 \Delta x^2}, \\ 
a_j^{n} &= \frac{-2 T^{n}_j}{\Delta x^2}, \\
b_j^{n} &=  \frac{\partial_x V^{n}_{j+1}}{2\Delta x} + \frac{T^{n}_{j+1} - T^{n}_{j-1}}{4 \Delta x^2}, \\
c_j^{n} &= -\frac{\partial_x V^{n}_{j-1}}{2\Delta x}  - \frac{T^{n}_{j+1} - T^{n}_{j-1}}{4 \Delta x^2}, \numberthis
\end{align*}
With these definitions, the Crank-Nicolson scheme can be written down as follows:
\begin{multline}
 -\frac{\Delta t}{2}a_j^{n+1}P_{j-1}^{n+1} + \left (1 - \frac{\Delta t}{2}b_j^{n+1} \right) P_j^{n+1} - \frac{\Delta t}{2} c_j^{n+1} P_{j+1}^{n+1} \\
 = a_j^n P_{j-1}^{n}
+ \left (1 + \frac{\Delta t}{2}b_j^n \right) P_j^{n}  + \frac{\Delta t}{2} c_j^n P_{j+1}^{n}.
\end{multline}
This equation can be written in matrix form by defining the following matrices
\begin{equation}
A =
\begin{bmatrix}
	a_0^{n+1} & b_1^{n+1} & 0                & 0                & 0        & \dots                                 & 0        \\
	c_0^{n+1} & a_1^{n+1} & b_2^{n+1} & 0                & 0        & \dots                                 & 0        \\
	0                & c_1^{n+1} & a_2^{n+1} & b_3^{n+1}   & 0        & \dots                                 & 0        \\
			     &                   &                   &                  &           &                                         &           \\
	\vdots         & \vdots         & \ddots         & \ddots         & \ddots & \vdots                              & \vdots \\
			     &                   &                   &                  &           &                                         &           \\
			     &                   &                   &                  &           &                                         &           \\
	0                &                   & \dots           &                   &  c_{J-2}^{n+1} & a_{J-1}^{n+1}  & b_J^{n+1} \\
	0                &                   & \dots           &                   &                         &  c_{J-1}^{n+1} & a_J^{n+1}
\end{bmatrix}
,\quad P^{n+1} =
\begin{bmatrix}
P_0^{n+1}       \\
P_1^{n+1}	     \\
                        \\
                        \\
\vdots               \\
                        \\
                        \\
P_{J-1}^{n+1} \\
P_J^{n+1}
\end{bmatrix}
\label{eqn:matrix}
\end{equation}

\begin{equation}
B =
\begin{bmatrix}
	a_0^{n} & b_1^{n}     & 0                 & 0          & 0                    & \dots            & 0        \\
	c_0^{n} & a_1^{n}     & b_2^{n}      & 0          & 0                    & \dots            & 0        \\
	0                & c_1^{n} & a_2^{n}      & b_3{n} & 0                    & \dots            & 0        \\
			     &               &                   &             &                      &                     &           \\
	\vdots         & \vdots     & \ddots         & \ddots   & \ddots            & \vdots           & \vdots \\
			     &               &                   &             &                      &                     &           \\
			     &               &                   &             &                      &                     &           \\
	0                &               & \dots           &             &  c_{J-2}^{n} & a_{J-1}^{n}  & b_J^{n} \\
	0                &               & \dots           &             &                     &  c_{J-1}^{n} & a_J^{n}
\end{bmatrix}
,\quad P^{n} =
\begin{bmatrix}
P_0^{n}       \\
P_1^n          \\
                    \\
                    \\
\vdots           \\
                    \\
                    \\
P_{J-1}^{n} \\
P_J^{n}
\end{bmatrix}
\end{equation}
With these matrices, the equation now becomes,
\begin{equation}
\left (\mathbb{1} - \frac{\Delta t}{2}A \right) \cdot P^{n+1} = \left (\mathbb{1} + \frac{\Delta t}{2}B \right) \cdot P^n,
\end{equation}
we interpret this equation as saying that half of a backwards Euler step acting on $P^{n+1}$ is equal to half of a forward Euler step acting on $P^n$. We write the equation to step $P$ forward one time step as
\begin{equation}
P^{n+1} = \frac{\mathbb{1} + \frac{\Delta t}{2}B}{\mathbb{1} - \frac{\Delta t}{2}A} P^n.
\end{equation}
Each time that we step forward using this equation we will be out by a factor, this means that at each step we will need to renormalize using the equation $\int P(x) dx = 1$. If we do not apply this normalization, then the norm of our vector will change dramatically during a simulation. In order to estimate the integrals involved, we used Simpson's rule of integration.

Likewise, we can apply the Crank-Nicolson scheme to the heat equation, (eq \ref{eqn:dimensionlessHeat}), by looking at the right-hand side of this equation, we find that

\begin{equation}
F_j^i = -\alpha \left (P_j^i (\partial_x V_j^i)^2 + T_j^i \frac{P_{j+1}^i - P_{j-1}^i }{2 \Delta x}\partial_x V_j^i  \right) + \beta \frac{T_{j+1}^i - 2T_j^i + T_{j-1}^i}{\Delta x^2}.
\end{equation}

Just like the discretized Smoluchowski equation, these equations can be written in matrix form. The temperature is normalized by assuming that the energy remains fixed, this will be true as long as no heat or current flows through the boundaries, i.e. $J(x=a) = 0 = J(x = b)$ and $\frac{\partial T}{\partial x} \rvert_a = 0 = \frac{\partial T}{\partial x} \rvert_b$. In this case, the energy is constant and is given by $E = \int P(x) V(x) dx + c_p \int T(x) dx$, so each time that we step the temperature forward, we have to calculate the potential and thermal energy and then scale the temperature so that the total energy remains fixed.

Fortunately the matrices that we are dealing with are very sparse, so the program used to solve these equations can save on memory by calling sparse matrix libraries.

Implementing these boundary conditions requires that we change our matrices $A$ and $B$. Specifically, we will need to modify the top and bottom rows. The reason that we can do this is because a matrix represents a system of equations. The first and last rows of the matrices effect the first and last components of the vectors that we are doing finite differences on. For each boundary condition, we need to make sure that the equations represented by our first and last rows reflect our desired boundary conditions.

\subsubsection{Dirichlet boundary conditions}
Imagine that we have a $n \times 1$ vector $u$ that we want to step forward with finite differences while keeping the first and last components of $u$ fixed. The equation for stepping $u$ forward is given by

\begin{equation}
u^{n+1} = \frac{\mathbb{1} + \frac{\Delta t}{2} B}{\mathbb{1} - \frac{\Delta t}{2} A} u^n,
\end{equation}
this equation will certainly affect the first and last components of $u$. In order to avoid this, we need to make sure that the first and last rows of $(\mathbb{1} + \frac{\Delta t}{2} B)$ and $(\mathbb{1} - \frac{\Delta t}{2} A)$ are the same as the first and last rows of the identity matrix.  One must take care when using the energy to normalize when considering Dirichlet boundary conditions, because the energy in the system is no longer conserved. To account for this, we calculate the heat flowing through the boundaries at each step and subtract this from the energy of the system before we normalized the system.

\subsubsection{Neumann boundary conditions}
The derivative at the boundaries can be made to be zero at the boundaries by making sure that the first and last rows of our matrices represent the equations $u_J - u_{J-1} = 0$ and $u_0 - u_1 = 0$ respectively. This can be done by replacing the first row with $\{ 1, \, -1, \, 0, \, \dots \, , \, 0 \}$ and the last row with $\{0, \, 0, \, \dots, \, 0, \, -1, \, 1 \}$.

\subsubsection{Periodic boundary conditions}
Periodic boundary conditions state that $u_0 = u_J$, if this is true at step $i$, then we can make sure that it is true at step $i+1$ by modifying our matrix appropriately. To be more precise if our finite differences matrix takes the form of equation \ref{eqn:matrix}, then in order to have periodic boundary conditions we would have to modify it to read:

\begin{equation}
A =
\begin{bmatrix}
	a_0^{n+1} & b_1^{n+1} & 0                & 0                & 0        & \dots                                 & c_0^{n+1}        \\
	c_0^{n+1} & a_1^{n+1} & b_2^{n+1} & 0                & 0        & \dots                                 & 0        \\
	0                & c_1^{n+1} & a_2^{n+1} & b_3^{n+1}   & 0        & \dots                                 & 0        \\
			     &                   &                   &                  &           &                                         &           \\
	\vdots         & \vdots         & \ddots         & \ddots         & \ddots & \vdots                              & \vdots \\
			     &                   &                   &                  &           &                                         &           \\
			     &                   &                   &                  &           &                                         &           \\
	0                &                   & \dots           &                   &  c_{J-2}^{n+1} & a_{J-1}^{n+1}  & b_J^{n+1} \\
	b_J^{n+1}                &                   & \dots           &                   &                         &  c_{J-1}^{n+1} & a_J^{n+1}
\end{bmatrix}.
\end{equation}
This is equivalent to stretching out the matrix in a periodic fashion, we note that the matrix for periodic boundary conditions cannot be written in tridiagonal form. When the matrices involved are tridiagonal, we can inform the compiler of our program so that it can used highly optimized routines for solving the equations. In the case of periodic boundary conditions, however, we simply inform the compiler that the matrices are sparse, this means that we do not get the same speed for periodic boundary conditions as we do for other boundary conditions.

All of these boundary conditions were implemented in Julia which is a high level open source language for scientific computing \cite{Bezanson2014}. A function was made so that the user can decide which boundary condition that they want through a keyword argument and then the program will implement finite differences for that boundary type. An example of finite differencing with periodic boundary conditions is shown in Figure \ref{fig:periodicSimulation}.
\begin{figure}
%	\center
%	\textbf{Periodic boundary conditions}
%	\vspace{0.5cm}
	\begin{subfigure}{0.49\textwidth}
		\includegraphics[width=\textwidth]{PeriodicInit}
	\end{subfigure}
	\begin{subfigure}{0.49\textwidth}
		\includegraphics[width=\textwidth]{PeriodicFinal}
	\end{subfigure}
	\caption{\textbf{An example of a periodic system visualized in one dimension.} A simulation of equations \ref{eqn:dimensionlessSmoluchowski} and \ref{eqn:dimensionlessHeat} using finite differences, $\Delta t = 1 \times 10^{-4}$ and the number of points on the x-axis is 1000, all quantities are dimensionless, this simulation involved 30,000 steps. We have $\alpha = 5 \times 10^{-3}$ and $\beta = 1 \times 10^{-2}$ we are imposing periodic boundary conditions on both the temperature and the probability distribution. \label{fig:periodicSimulation}}
\end{figure}

\section{Testing the numerics}

The idea behind finite differences is that as the discretization size goes to zero, the numerical approximation should converge on the correct analytical solution. Here we will compare our numerics with some known analytical results as well as performing convergence tests.
\subsection{A comparison with analytical results}

\begin{figure}
	\begin{subfigure}{0.49\textwidth}
	\includegraphics[width=\columnwidth]{smoluchowskiAnalytic}
	\end{subfigure}
	\begin{subfigure}{0.49\textwidth}
	\includegraphics[width=\columnwidth]{smoluchowskiAnalyticNormDiff}
	\end{subfigure}
	\caption{\textbf{Finite differences in the steady state.} The steady state for the system is given by the Boltzmann distribution, we start the system off in this state and then simulate the system forward 50,000 steps with $\Delta t = 10^{-4}$ and $\Delta x = 2 \times 10^{-2}$. (a) The analytical steady state with the state after the simulation. (b) The absolute difference between these two vectors. Even after 50,000 steps the system has not deviated from the analytical steady state significantly. \label{fig:smoluchowskiCompare}}
\end{figure}

For a confining potential and with $\alpha = 0$, the Smoluchowski equation (equation \ref{eqn:dimensionlessSmoluchowski}) has a steady state probability density given by the Boltzmann distribution.
Figure \ref{fig:smoluchowskiCompare} shows the a simulation where we began the system in the analytically calculated steady state, we then used finite differences to step forward 50,000 steps with $\Delta t = 1 \times 10^{-4}$. After this simulation, we only found a minimal divergence from the steady state.

The heat equation (equation \ref{eqn:dimensionlessHeat}) can be solved using a Fourier series technique. In the case where there are no sources of heat, we have
\begin{equation}
\frac{\partial T}{\partial t} = \beta \frac{\partial^2 T}{\partial x^2}.
\end{equation}
The initial condition for the temperature will be denoted by,
\begin{equation}
T(x, 0) = f(x).
\end{equation}
Given boundaries at $x=\pm \infty$ with vanishing $\frac{\partial T(x, \, t)}{\partial x}$, the time-dependent solution of the heat equation can be given as:

\begin{equation}
T(x, t) = \sum_{n=1}^\infty D_n \sin \left(\frac{n \pi x}{L} \right) \exp\left(\frac{-n^2 \pi^2 \beta t}{L^2}\right), \label{eqn:analTemperature}
\end{equation}
where
\begin{equation}
D_n = \frac{2}{L} \int_0^L f(x) \sin \left(\frac{n \pi x}{L} \right) dx.
\end{equation}
Say that we begin with a Gaussian function for the temperature, such that
\begin{equation}
T(x, \, 0) = f(x) =  \exp{\left (-\frac{x^2}{4 \beta} \right)},
\end{equation}
then as time goes on, the solution will be given by
\begin{equation}
T(x, \, t) = \frac{1}{\sqrt{t + 1}} \exp{ \left(-\frac{x^2}{4 \beta (t + 1)} \right)}.
\end{equation}
We can compare these analytical results to the numerical ones obtained through finite differences, this is done in Figures ~\ref{fig:smoluchowskiCompare} and \ref{fig:temperatureCompare}. These figures show that even after many steps, the calculated solution has not deviated from the analytical one.
\begin{figure}
	\center
	\begin{subfigure}{0.45\textwidth}
	\includegraphics[width=\columnwidth]{temperatureAnalytic}
	\end{subfigure}
	\begin{subfigure}{0.45\textwidth}
	\includegraphics[width=\columnwidth]{temperatureAbsDiff}
	\end{subfigure}
	\caption{\textbf{Finite differences on the heat equation.} We simulate a situation where there are no sources for the heat equation and use equation \ref{eqn:analTemperature} to obtain the analytical result we then compare this to the result of finite differences with $\Delta t = 1 \times 10^{-2}$ and $\Delta x = 2 \times 10^{-2}$. \label{fig:temperatureCompare}}
\end{figure}
%Lets say that the boundaries are at $x = \pm \infty$ and that the derivative of the temperature is zeros at the boudaries. The solution to the heat equation is given by:

\subsection{Convergence tests}
%Here we will decrease the step size and see whether or not the numerical scheme converges on a particular result

\begin{figure}
	\begin{subfigure}{0.49\textwidth}
		\includegraphics[width=\columnwidth]{probabilityConvergence}
	\end{subfigure}
	\begin{subfigure}{0.49\textwidth}
		\includegraphics[width=\columnwidth]{probabilityConvergenceRate}
	\end{subfigure}
\caption{\textbf{The convergence of the probability distribution.} As $\Delta t$ is decreased, the spatial discretization $\Delta x$ is kept constant at $6 \times 10^{-3}$. (a) The Smoluchoski equation is simulated forward for 1.0 seconds each line shows the result with a different value of the time step $\Delta t$. (b) The normed difference between each of the successive vectors is calculated and the result is plotted on  a log scale, the slope of this graph is called the convergence rate.}
\label{fig:Convergence}
\end{figure}

If the finite differences methods that we have implemented are correct, then as the discretization size goes to zero, the numerical approximation will converge on the correct solution to the underlying equation being approximated. In the previous section we showed that finite differences approximated the solution very closely in some instances where the analytical result could be obtained. In general, we will not have an analytical solution to compare to but we would still like to be able to quantify the performance of our techniques.

Convergence tests involve decreasing the discretization size and checking whether the numerical solutions converge at all. In Figure \ref{fig:Convergence}, the Smoluchowski equation was simulated while keeping the temperature fixed, each time we halve $\Delta t$ and measure the normed difference between the new result and the previous one.

Likewise, we can do convergence tests for the coupled system, for brevity we have only included the results for the evolution of the temperature. All of these plots show that the numerical solution converges exponentially. The convergence tests themselves do not prove that the numerical algorithms properly represent the equations \ref{eqn:dimensionlessSmoluchowski} and \ref{eqn:dimensionlessHeat}, however, combined with our analytical tests, they give us confidence that the numerics do agree with the true solution.

\begin{figure}
	\begin{subfigure}{0.49\textwidth}
		\includegraphics[width=\columnwidth]{temperatureConvergence}
	\end{subfigure}
	\begin{subfigure}{0.49\textwidth}
		\includegraphics[width=\columnwidth]{temperatureConvergenceRate}
	\end{subfigure}
\caption{\textbf{The convergence of the temperature.} As $\Delta t$ is decreased, the spatial discretization the number of points are kept constant at 2500. (a) the coupled equations are simulated forward for 2.0 seconds, each line shows the temperature with a different value of the time step $\Delta t$. (b) the normed difference between each of the successive vectors is calculated and the result is plotted on  a log scale.}
\label{fig:temperatureConvergence}
\end{figure}

One can also perform convergence tests by increasing the number of points, from this one learns how sensitive the numerics are to the size of the spatial discretization $\Delta x$. This is shown in Figure \ref{fig:spaceConvergence}, here we see that the temperature converges much more rapidly than the probability distribution does, however as $\Delta x$ goes down, both converge exponentially.

\begin{figure}
	\center{
	\begin{subfigure}{0.5\textwidth}
		\includegraphics[width=\columnwidth]{spaceConvergenceInit}
		\caption{}
	\end{subfigure}
	}

	\begin{subfigure}{0.49\textwidth}
		\includegraphics[width=\columnwidth]{spaceConvergenceDensity}
		\caption{}
	\end{subfigure}
	\begin{subfigure}{0.49\textwidth}
		\includegraphics[width=\columnwidth]{spaceConvergenceTemperature}
		\caption{}
	\end{subfigure}

	\begin{subfigure}{0.49\textwidth}
		\includegraphics[width=\columnwidth]{densityConvergenceSpace}
		\caption{}
	\end{subfigure}
	\begin{subfigure}{0.49\textwidth}
		\includegraphics[width=\columnwidth]{temperatureConvergenceSpace}
		\caption{}
	\end{subfigure}
\caption{\textbf{Convergence tests for $\Delta x$.} $\Delta t$ is held fixed at $1 \times 10^{-5}$ and $\alpha = 5 \times 10^{-4}$ and $\beta = 1 \times 10^{-2}$. (a) The initial configuration of the system, both the potential and the initial temperature are chosen to be functions with large derivatives so that we can see how sensitive the numerics are to gradients. (b, c) The system after 40,000 steps forward in time, here we have not used the filling plot style to show the probability distribution so that one can see the difference between the different lines. (d) For each vector that was found through finite differences, we calculate its difference from the previous one, the $x$ axis shows the length of each of the vectors. The $y$ axis shows the log of these differences (e) Convergence of the temperature with an increasing number of points. \label{fig:spaceConvergence}}
\end{figure}