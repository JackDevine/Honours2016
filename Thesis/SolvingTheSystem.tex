\section{Steady state solution} \label{SteadyState}
In order to see how the equations of motion behave with time, we have to resort to numerical methods (see section \ref{numerics}). However we note that for a given potential there will be a stationary solution that we will refer to as the ``steady state'', given periodic boundary conditions, we will derive an analytical form for this steady state.

In the steady state, we have:

\begin{eqnarray}
\frac{\partial P(x, t)}{\partial t} &=&  0 \ = \ \frac{\partial J}{\partial x} \label{eqn:SmoluchowskiSteady} \\
\frac{\partial T(x, t)}{\partial t} &=& 0 \ = \ -\kappa q(x,t) + \frac{\partial}{\partial x} \left ( D \frac{\partial T(x, t)}{\partial x} \right ) \label{eqn:TemperatureSteady}
\end{eqnarray}

Suppose that we have the following boundary conditions:
\begin{align}
P(x = 0) & = P(x = L) \\
J(x = 0) & = J(x = L) \\
\left. \frac{\partial T}{\partial x} \right \rvert_{x = 0} & = 0 = \left. \frac{\partial T}{\partial x} \right \rvert_{x = L} \label{eqn:temperatureBoundary}
\end{align}
where $L$ is the length scale of the system. Physically, these conditions say that the nett current flowing out of the boundaries is zero and that no heat escapes from the system, thus the energy of the sytem is conserved. Section 5.2 of \cite{Gardiner2009} gives the steady state current as:

\begin{equation}
J_s = \left [\frac{2 k_B T(L)}{\psi(L)} - \frac{2 k_B T(0)}{\psi(0)}  \right] P_s(0) \left [\int_0^L dx'/\psi(x') \right]^{-1}
\label{eqn:SteadyCurrent}
\end{equation}

with $\psi(x) \equiv \exp[-\int_0^x dx' \frac{\partial_x V(x')}{2 k_B T(x')}]$. Meanwhile, the density is:

\begin{equation}
P_s(x) = P_s(0) \left [\frac{\int_0^x \frac{dx'}{\psi(x')} \frac{T(L)}{\psi(L)} + \int_x^L \frac{dx'}{\psi(x')} \frac{T(0)}{\psi(0)} }{\frac{T(x)}{\psi(x)} \int_0^L \frac{dx'}{\psi(x')} } \right]
\label{eqn:SteadyDensity}
\end{equation}
In this case, $J_s$ is a constant and $P_s(0)$ is also a constant. Assuming that we know these constants it is now possible to find the steady state temperature. We have:

\begin{equation}
\frac{\partial T}{\partial t} = 0 = -\kappa J_s \partial_x V + D \frac{\partial^2 T}{\partial x^2} \label{eqn:steadyTemperatureODE}
\end{equation}

In one dimension, \ref{eqn:steadyTemperatureODE} can be written as an ordinary differential equation of the form
\begin{equation}
T''(x) = \frac{\kappa J_s}{D} V'(x)
\end{equation}

We can solve this equation by integrating both sides twice to give:

\begin{equation}
T(x) = \frac{\kappa J_s}{D} \int_0^x V(x') dx' + \xi x + d \label{eqn:steadyTemperature}
\end{equation}
for unkown constants $\xi$ and $d$. By applying the boundary condition \ref{eqn:temperatureBoundary}, we find that
\begin{align}
T'(0) & = 0 = \frac{\kappa J_s}{D} V(0) + \xi \\
T'(L) & = 0 = \frac{\kappa J_s}{D} V(L) + \xi
\end{align}
This implies that either $J_s = 0$ or $V(0) = V(L)$, meaning that the coupled system does not admit a steady state solution for a tilted potential. Later, we will see that it is possible to have a steady state in higher dimensions where the flow of heat from the environment can dissipate the heat produced by the Brownian particle. By recalling that $E = \int_0^L P(x') V(x') dx' + c \int_0^L T(x') dx'$ we are able to find an expression for $d$. First, we will integrate the temperature from $0$ to $L$
\begin{equation}
\int_0^L T(x') dx' = \frac{\kappa J_s}{D} \int_0^L dx' \int_0^{x'} V(x'') dx'' + \xi \frac{L^2}{2} + d L
\end{equation}
Therefore,
\begin{equation}
	d = \frac{1}{L} \left(c E - c\int_0^L P(x') V(x') dx' - \frac{\kappa J_s}{D} \int_0^L dx' \int_0^{x'} V(x'') d x'' + \xi \frac{L^2}{2} \right)
\end{equation}

It would seem that one should be able to calculate the steady state current and density directly from the equations shown above. However, we notice that the constants $J_s$ and $P_s(0)$ have to satisfy equations (\ref{eqn:SteadyCurrent}), (\ref{eqn:SteadyDensity}) and (\ref{eqn:steadyTemperature}) while also satisfying the normalization condition $\int_0^L P(x) dx = 1$. To do this we define an objective function given by

\begin{equation}
obj(J_s, P_s(0)) = \left (J_s - \left [\frac{2 k_B T(L)}{\psi(L)} - \frac{2 k_B T(0)}{\psi(0)}  \right] P_s(0) \left [\int_0^L dx'/\psi(x') \right]^{-1} \right)^2  \label{eqn:Objective}
\end{equation}

And we minimize this objective function with respect to $J_s$ and $P_s(0)$ under the constraint $\int_0^L P(x) dx = 1$. Another way to do this is to guess a steady state density and temperature and use finite differencing to simulate forward in time until the transients die out.

%----------------------------------------------------------------------------------------
%	NUMERICS
%----------------------------------------------------------------------------------------
\section{Finite differences}  \label{numerics}
The one dimensional equation can be solved on a discrete grid by using the finite differences method, the main idea behind this strategy is to approximate derivatives with equations of the form:

\begin{equation}
\frac{d f}{d x} \approx \frac{f(x - h) - f(x + h)}{2h}
\end{equation}

for some small $h$, likewise the second derivative of a function is approximated with
\begin{equation}
\frac{d^2 f}{d x^2} = \frac{f(x - h) - 2f(x) + f(x + h)}{h^2}
\end{equation}
In our simulations, we will use the Crank Nicolson scheme to solve the equations. From now on, we will use the notation that $F(j \Delta x, n \Delta t) = F_j^n$, the key equation for the Crank Nicolson scheme is:
\begin{equation}
\frac{P_j^{n+1} - P_j^n}{\Delta t} = \frac{1}{2}(F_j^{n+1} + F_j^n)
\end{equation}
where $F$ represents the right hand side of the equation that we are doing finite differences on. By applying finite differences to the dimensionless Smoluchowski equation (eq \ref{eqn:dimensionlessSmoluchowski}), we find that:
\begin{equation}
F_j^{i} = \frac{P^i_{j+1} \partial V^i_{j+1} - P^i_{j-1} \partial V^i_{j-1}}{2 \Delta x} + \frac{T^i_{j+1} - T^i_{j-1}}{2 \Delta x} \frac{P^i_{j+1} - P^i_{j-1}}{2 \Delta x} + T^i_j \frac{}{} \frac{P^i_{j+1}- 2P^i_j + P^i_{j-1}}{\Delta x^2} 
\end{equation}
where we omitted the hats for notational convenience.
We make the following definitions:
\begin{align*}
a_j^{n+1} &= \frac{-2 T^{n+1}_j}{\Delta x^2} \\
b_j^{n+1} &=  \frac{\partial_x V^{n+1}_{j+1}}{2\Delta x} + \frac{T^{n+1}_{j+1} - T^{n+1}_{j-1}}{4 \Delta x^2} \\
c_j^{n+1} &= -\frac{\partial_x V^{n+1}_{j-1}}{2\Delta x}  - \frac{T^{n+1}_{j+1} - T^{n+1}_{j-1}}{4 \Delta x^2} \\ 
a_j^{n} &= \frac{-2 T^{n}_j}{\Delta x^2} \\
b_j^{n} &=  \frac{\partial_x V^{n}_{j+1}}{2\Delta x} + \frac{T^{n}_{j+1} - T^{n}_{j-1}}{4 \Delta x^2} \\
c_j^{n} &= -\frac{\partial_x V^{n}_{j-1}}{2\Delta x}  - \frac{T^{n}_{j+1} - T^{n}_{j-1}}{4 \Delta x^2} \numberthis
\end{align*}
With these definitions, the Crank Nicolson scheme can be written down as follows:
\begin{multline}
 -\frac{\Delta t}{2}a_j^{n+1}P_{j-1}^{n+1} + \left (1 - \frac{\Delta t}{2}b_j^{n+1} \right) P_j^{n+1} - \frac{\Delta t}{2} c_j^{n+1} P_{j+1}^{n+1} = a_j^n P_{j-1}^{n}
+ \left (1 + \frac{\Delta t}{2}b_j^n \right) P_j^{n}  + \frac{\Delta t}{2} c_j^n P_{j+1}^{n}
\end{multline}
This equation can be written in matrix form by defining the following matrices:
\begin{equation}
A =
\begin{bmatrix}
	a_0^{n+1} & b_1^{n+1} & 0                & 0                & 0        & \dots                                 & 0        \\
	c_0^{n+1} & a_1^{n+1} & b_2^{n+1} & 0                & 0        & \dots                                 & 0        \\
	0                & c_1^{n+1} & a_2^{n+1} & b_3^{n+1}   & 0        & \dots                                 & 0        \\
			     &                   &                   &                  &           &                                         &           \\
	\vdots         & \vdots         & \ddots         & \ddots         & \ddots & \vdots                              & \vdots \\
			     &                   &                   &                  &           &                                         &           \\
			     &                   &                   &                  &           &                                         &           \\
	0                &                   & \dots           &                   &  c_{J-2}^{n+1} & a_{J-1}^{n+1}  & b_J^{n+1} \\
	0                &                   & \dots           &                   &                         &  c_{J-1}^{n+1} & a_J^{n+1}
\end{bmatrix}
,\quad P^{n+1} =
\begin{bmatrix}
P_0^{n+1}       \\
P_1^{n+1}	     \\
                        \\
                        \\
\vdots               \\
                        \\
                        \\
P_{J-1}^{n+1} \\
P_J^{n+1}
\end{bmatrix}
\end{equation}

\begin{equation}
B =
\begin{bmatrix}
	a_0^{n} & b_1^{n}     & 0                 & 0          & 0                    & \dots            & 0        \\
	c_0^{n} & a_1^{n}     & b_2^{n}      & 0          & 0                    & \dots            & 0        \\
	0                & c_1^{n} & a_2^{n}      & b_3{n} & 0                    & \dots            & 0        \\
			     &               &                   &             &                      &                     &           \\
	\vdots         & \vdots     & \ddots         & \ddots   & \ddots            & \vdots           & \vdots \\
			     &               &                   &             &                      &                     &           \\
			     &               &                   &             &                      &                     &           \\
	0                &               & \dots           &             &  c_{J-2}^{n} & a_{J-1}^{n}  & b_J^{n} \\
	0                &               & \dots           &             &                     &  c_{J-1}^{n} & a_J^{n}
\end{bmatrix}
,\quad P^{n} =
\begin{bmatrix}
P_0^{n}       \\
P_1^n          \\
                    \\
                    \\
\vdots           \\
                    \\
                    \\
P_{J-1}^{n} \\
P_J^{n}
\end{bmatrix}
\end{equation}
With these matrices the equation now becomes,
\begin{equation}
\left (\mathbb{1} - \frac{\Delta t}{2}A \right) \cdot P^{n+1} = \left (\mathbb{1} + \frac{\Delta t}{2}B \right) \cdot P^n
\end{equation}
we interpret this equation as saying that half of a backwards Euler step acting on $P^{n+1}$ is equal to half of a forward Euler step acting on $P^n$. We write the equation to step $P$ forward one time step as
\begin{equation}
P^{n+1} = \frac{\mathbb{1} + \frac{\Delta t}{2}B}{\mathbb{1} - \frac{\Delta t}{2}A} P^n
\end{equation}
Each time that we step forward using this equation we will be out by a factor, this means that at each step we will need to renormalize using the equation $\int P(x) dx = 1$.

Likewise, we can apply the Crank Nicolson scheme to the heat equation, (eq \ref{eqn:dimensionlessHeat}), by looking at the right hand side of this equation, we find that

\begin{equation}
F_j^i = -\alpha \left (P_j^i (\partial_x V_j^i)^2 + \frac{T_{j+1}^i - T_{j-1}^i }{2 \Delta x}\partial_x V_j^i  \right) + \beta \frac{T_{j+1}^i - T_j^i + T_{j-1}^i}{\Delta x^2}
\end{equation}

Just like the discretized Smoluchowski equation, these equations can be written in matrix form. The temperature is normalized by assuming that the energy remains fixed, this will be true as long as no heat or current flows through the boundaries, i.e. $J(x=a) = 0 = J(x = b)$ and $\frac{\partial T}{\partial x} \rvert_a = 0 = \frac{\partial T}{\partial x} \rvert_b$. In this case, the energy is constant and is given by $E = \int P(x) V(x) dx + c_p \int T(x) dx$, so each time that we step the temperature forward, we have to calculate the potential and thermal energy and then scale the temperature so that the total energy remains fixed.

Fortunately the matrices that we are dealing with are very sparse, so the program used to solve these equations can save on memory by calling sparse matrix libraries.

\subsection{Boundary conditions}
So far we have discussed how to solve the equations inside the domain of interest, however we our equations of motion will require that we prescribe boundary conditions that the system must satisfy. The boundary conditions that have been implemented for this project are as follows:

\begin{itemize}
\item{Dirichlet: The value of the solution is specified at the boundaries}
\item{Neumann: The value of the first derivative of the solution is specified at the boundaries}
\item{Periodic: The value at the boundaries is not specified, but the left and right boundaries must take on the same value}
\end{itemize}

Each type of boundary conditions comes along with its own physical interpretation, first we will deal with the boundary conditions imposed on the probability distribution. If the potential goes to infinity at the boundaries, then the probability distribution must vanish at the boundaries before applying boundary conditions at all, therefore no boundary conditions need to be applied. There are some potentials that are periodic where the value of the potential is the same finite number at both sides, in this case it can be interesting to apply periodic boundary conditions to the probability distribution. Physically, this corresponds to a system where a particle passing through the left side will appear at the right side and vice-versa. One can picture this by imagining a particle constrained to a circular domain.

As for the temperature, both Neumann and Dirichlet boundary conditions are realized. In the case of Neumann boundary conditions, the derivative was set to zero at both boundaries. This is a very interesting case to imagine, because the derivative of the temperature physically represents the flow of heat in our system. Therefore the physical interpretation of a vanishing derivative at the boundary is that no heat is flowing through the boundaries, or in other words the system is enclosed in a perfectly insulating box. This system conserves energy, which is convenient when one goes to renormalize the calculated temperature by using the energy of the system. The second type of boundary condition that can be imposed on the temperature is the Dirichlet type where the value of the temperature is specified at both boundaries. The physical interpretation of this requirement is that the system is embedded in a much larger system that is at a fixed temperature. The tacit assumption made is that no matter what the system does, it is not able to affect the temperature of the environment. One must take care when using the energy to renormalize when considering Dirichlet boundary conditions, this is because the energy in the system is no longer conserved. To account for this, we calculated the heat flowing through the boundaries at each step and subtracted this from the energy of the system before we renormalized the system.

Implementing these boundary conditions requires that we change our matrices $A$ and $B$, specifically we will need to modify the top and bottom rows. The reason that we can do this is because a matrix represents a system of equations, the first and last rows of the matrices effects the first and last components of the vectors that we are doing finite differences on. For each boundary condition, we need to make sure that the equations represented by our first and last rows reflect our desired boundary conditions.

\subsubsection{Dirichlet boundary conditions}
Imagine that we have a $n \times n$ vector $u$ that we want to step forward with finite differences while keeping the first and last components of $u$ fixed. The equation for stepping $u$ forward is given by:

\begin{equation}
u^{n+1} = \frac{\mathbb{1} + \frac{\Delta t}{2} B}{\mathbb{1} - \frac{\Delta t}{2} A} u^n
\end{equation}

This equation will certainly affect the first and last components of $u$, in order to avoid this, we need to make sure that the first and last rows of $(\mathbb{1} + \frac{\Delta t}{2} B)$ and $(\mathbb{1} - \frac{\Delta t}{2} A)$ are the same as the first and last rows of the identity matrix.

\subsubsection{Neumann boundary conditions}
The derivative at the boundaries can be made to be zero at the boundaries by making sure that the first and last rows of our matrices represent the equations $u_J - u_{J-1} = 0$ and $u_0 - u_1 = 0$ respectively.

\subsubsection{Periodic boundary conditions}

All of these boundary conditions were implemented in Julia \cite{Bezanson2014}, the user can decide which boundary condition that they want through a keyword argument and then the program will implement finite differences for that boundary type.

\section{Testing the numerics}

The idea behind finite differences is that as the discretization size goes to zero, the numerical approximation should converge on the correct analytical solution. We will compare our numerics with some known analytical results as well as performing convergence tests.
\subsection{A comparison with analytical results}

The Smoluchowski equation has a steady state probability density that takes on the form
\begin{equation}
P_{ss}(x) = N \exp{\left(-\int_a^x \frac{V'(x')}{T(x')} dx' \right)} \label{eqn:analSteadyState}
\end{equation}

Figure \ref{fig:smoluchowskiCompare} shows the a simulation where we began the system in the analytically calculated steady state, we then used finite differences to step forward 50,000 steps with $\Delta t = 10^{-4}$. After this simulation, we only found a minimal divergence from the steady state.
\begin{figure}
	\begin{subfigure}{0.49\textwidth}
	\includegraphics[width=\columnwidth]{smoluchowskiAnalytic}
	\end{subfigure}
	\begin{subfigure}{0.49\textwidth}
	\includegraphics[width=\columnwidth]{smoluchowskiAnalyticNormDiff}
	\end{subfigure}
	\caption{\textbf{Finite differences in the steady state.} The steady state for the system was calculated using equation ~\ref{eqn:analSteadyState}, we start the system off in this state and then simulate the system forward 50,000 steps with $\Delta t = 10^{-4}$ and $\Delta x = 0.02$. (a) shows the analytical steady state with the state after the simulation, (b) shows the absolute difference between these two vectors. Even after 50,000 steps the system has not deviated from the analytical steady state significantly. \label{fig:smoluchowskiCompare}}
\end{figure}

The heat equation can be solved using a Fourier series technique
\begin{equation}
\frac{\partial T}{\partial t} = \beta \frac{\partial^2 T}{\partial x^2}
\end{equation}
the initial condition for the temperature will be denoted by,
\begin{equation}
T(x, 0) = f(x)
\end{equation}

Lets say that the boundaries are at $x = \pm \infty$ and that the derivative of the temperature is zeros at the boudaries. The solution to the heat equation is given by:

\begin{equation}
T(x, t) = \sum_{n=1}^\infty D_n \sin \left(\frac{n \pi x}{L} \right) \exp\left(\frac{-n^2 \pi^2 \beta t}{L^2}\right)  \label{eqn:analTemperature}
\end{equation}
where
\begin{equation}
D_n = \frac{2}{L} \int_0^L f(x) \sin \left(\frac{n \pi x}{L} \right) dx
\end{equation}
We can compare these analytical results to the numerical ones obtained through finite differences, this is done in Figure ~\ref{fig:smoluchowskiCompare}

\begin{figure}
	\begin{subfigure}{0.49\textwidth}
	\includegraphics[width=\columnwidth]{temperatureAnalytic}
	\end{subfigure}
	\begin{subfigure}{0.49\textwidth}
	\includegraphics[width=\columnwidth]{temperatureAbsDiff}
	\end{subfigure}
	\caption{\textbf{Finite differences on the heat equation.} We simulate a situation where there are no sources for the heat equation and use equation \ref{eqn:analTemperature} to obtain the analytical result we then compare this to the result of finite differences with $\Delta t = 10^{-2}$ and $\Delta x = 0.02$. \label{fig:temperatureCompare}}
\end{figure}

\subsection{Convergence tests}
The idea of finite differences is that as the step size goes to zero, the numerical approximation will converge on the correct solution to the underlying equation being approximated. In the previous section we showed that finite differences approximated the solution very closely in some instances where the analytical result could be obtained. In general, we will not have an analytical solution to compare to but we would still like to be able to quantify the performance of our techniques.

Convergence tests involve decreasing the discretization size and checking whether the numerical solutions converge at all. In Figure \ref{fig:Convergence}, the Smoluchowski equation was simulated while keeping the temperature fixed, each time we halve $\Delta t$ and measure the normed difference between the new result and the previous one.

%Here we will decrease the step size and see whether or not the numerical scheme converges on a particular result

\begin{figure}
	\begin{subfigure}{0.49\textwidth}
		\includegraphics[width=\columnwidth]{probabilityConvergence}
	\end{subfigure}
	\begin{subfigure}{0.49\textwidth}
		\includegraphics[width=\columnwidth]{probabilityConvergenceRate}
	\end{subfigure}
\caption{\textbf{The convergence of the probability distribution.} As $\Delta t$ is decreased, the spatial discretization $\Delta x$ is kept constant at 0.006. (a) the Smoluchoski equation is simulated forward for 1.0 seconds each line shows the result with a different value of the time step $\Delta t$. (b) the normed difference between each of the successive vectors is calculated and the result is plotted on  a log scale, the slope of this graph is called the convergence rate.}
\label{fig:Convergence}
\end{figure}

Likewise, we can do convergence tests for the coupled system, for brevity we have only included the results for the evolution of the temperature. All of these plots show that the approximation converges exponentially to the solution.

\begin{figure}
	\begin{subfigure}{0.49\textwidth}
		\includegraphics[width=\columnwidth]{temperatureConvergence}
	\end{subfigure}
	\begin{subfigure}{0.49\textwidth}
		\includegraphics[width=\columnwidth]{temperatureNormDiff}
	\end{subfigure}
\caption{\textbf{The convergence of the temperature.} As $\Delta t$ is decreased, the spatial discretization $\Delta x$ is kept constant at 0.006. (a) the coupled equations are simulated forward for 1.0 seconds each line shows the temperature with a different value of the time step $\Delta t$. (b) the normed difference between each of the successive vectors is calculated and the result is plotted on  a log scale.}
\label{fig:temperatureConvergence}
\end{figure}